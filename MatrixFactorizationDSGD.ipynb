{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iDU2ONH3rfEY",
    "outputId": "abb98cb6-8ad8-4775-fb43-c0337d049bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 ; error = 4.7974\n",
      "Iteration: 2 ; error = 4.3509\n",
      "Iteration: 3 ; error = 3.4138\n",
      "Iteration: 4 ; error = 2.7751\n",
      "Iteration: 5 ; error = 2.3856\n",
      "Iteration: 6 ; error = 2.0542\n",
      "Iteration: 7 ; error = 1.8083\n",
      "Iteration: 8 ; error = 1.5854\n",
      "Iteration: 9 ; error = 1.3549\n",
      "Iteration: 10 ; error = 1.1946\n",
      "Iteration: 11 ; error = 0.9581\n",
      "Iteration: 12 ; error = 0.7756\n",
      "Iteration: 13 ; error = 0.6333\n",
      "Iteration: 14 ; error = 0.4947\n",
      "Iteration: 15 ; error = 0.4250\n",
      "Iteration: 16 ; error = 0.3744\n",
      "Iteration: 17 ; error = 0.3562\n",
      "Iteration: 18 ; error = 0.3079\n",
      "Iteration: 19 ; error = 0.2972\n",
      "Iteration: 20 ; error = 0.2904\n",
      "Iteration: 21 ; error = 0.2799\n",
      "Iteration: 22 ; error = 0.2679\n",
      "Iteration: 23 ; error = 0.2949\n",
      "Iteration: 24 ; error = 0.2819\n",
      "Iteration: 25 ; error = 0.2689\n",
      "Iteration: 26 ; error = 0.2601\n",
      "Iteration: 27 ; error = 0.2570\n",
      "Iteration: 28 ; error = 0.2682\n",
      "Iteration: 29 ; error = 0.2764\n",
      "Iteration: 30 ; error = 0.2770\n",
      "Iteration: 31 ; error = 0.2476\n",
      "Iteration: 32 ; error = 0.2785\n",
      "Iteration: 33 ; error = 0.2459\n",
      "Iteration: 34 ; error = 0.2410\n",
      "Iteration: 35 ; error = 0.2536\n",
      "Iteration: 36 ; error = 0.2607\n",
      "Iteration: 37 ; error = 0.2353\n",
      "Iteration: 38 ; error = 0.2422\n",
      "Iteration: 39 ; error = 0.2382\n",
      "Iteration: 40 ; error = 0.2703\n",
      "Iteration: 41 ; error = 0.2450\n",
      "Iteration: 42 ; error = 0.2455\n",
      "Iteration: 43 ; error = 0.2468\n",
      "Iteration: 44 ; error = 0.2305\n",
      "Iteration: 45 ; error = 0.2395\n",
      "Iteration: 46 ; error = 0.2383\n",
      "Iteration: 47 ; error = 0.2363\n",
      "Iteration: 48 ; error = 0.2366\n",
      "Iteration: 49 ; error = 0.2418\n",
      "Iteration: 50 ; error = 0.2444\n",
      "Iteration: 51 ; error = 0.2353\n",
      "Iteration: 52 ; error = 0.2540\n",
      "Iteration: 53 ; error = 0.2597\n",
      "Iteration: 54 ; error = 0.2496\n",
      "Iteration: 55 ; error = 0.2239\n",
      "Iteration: 56 ; error = 0.2373\n",
      "Iteration: 57 ; error = 0.2400\n",
      "Iteration: 58 ; error = 0.2318\n",
      "Iteration: 59 ; error = 0.2369\n",
      "Iteration: 60 ; error = 0.2494\n",
      "Iteration: 61 ; error = 0.2538\n",
      "Iteration: 62 ; error = 0.2537\n",
      "Iteration: 63 ; error = 0.2420\n",
      "Iteration: 64 ; error = 0.2463\n",
      "Iteration: 65 ; error = 0.2332\n",
      "Iteration: 66 ; error = 0.2370\n",
      "Iteration: 67 ; error = 0.2406\n",
      "Iteration: 68 ; error = 0.2440\n",
      "Iteration: 69 ; error = 0.2224\n",
      "Iteration: 70 ; error = 0.2374\n",
      "Iteration: 71 ; error = 0.2257\n",
      "Iteration: 72 ; error = 0.2350\n",
      "Iteration: 73 ; error = 0.2311\n",
      "Iteration: 74 ; error = 0.2433\n",
      "Iteration: 75 ; error = 0.2361\n",
      "Iteration: 76 ; error = 0.2366\n",
      "Iteration: 77 ; error = 0.2358\n",
      "Iteration: 78 ; error = 0.2357\n",
      "Iteration: 79 ; error = 0.2196\n",
      "Iteration: 80 ; error = 0.2369\n",
      "Iteration: 81 ; error = 0.2280\n",
      "Iteration: 82 ; error = 0.2243\n",
      "Iteration: 83 ; error = 0.2226\n",
      "Iteration: 84 ; error = 0.2238\n",
      "Iteration: 85 ; error = 0.2155\n",
      "Iteration: 86 ; error = 0.2343\n",
      "Iteration: 87 ; error = 0.2249\n",
      "Iteration: 88 ; error = 0.2227\n",
      "Iteration: 89 ; error = 0.2270\n",
      "Iteration: 90 ; error = 0.2305\n",
      "Iteration: 91 ; error = 0.2316\n",
      "Iteration: 92 ; error = 0.2229\n",
      "Iteration: 93 ; error = 0.2158\n",
      "Iteration: 94 ; error = 0.2163\n",
      "Iteration: 95 ; error = 0.2272\n",
      "Iteration: 96 ; error = 0.2137\n",
      "Iteration: 97 ; error = 0.2086\n",
      "Iteration: 98 ; error = 0.2178\n",
      "Iteration: 99 ; error = 0.2206\n",
      "Iteration: 100 ; error = 0.2170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 4.797378500696788),\n",
       " (1, 4.350850007942864),\n",
       " (2, 3.413835364467555),\n",
       " (3, 2.7751171382961335),\n",
       " (4, 2.3855512095787725),\n",
       " (5, 2.0541702493122957),\n",
       " (6, 1.8082520878869468),\n",
       " (7, 1.5854033433628856),\n",
       " (8, 1.35489638874836),\n",
       " (9, 1.1945649105850746),\n",
       " (10, 0.9581458741763222),\n",
       " (11, 0.7755559191717589),\n",
       " (12, 0.6332562253697951),\n",
       " (13, 0.494677465171062),\n",
       " (14, 0.425007351759414),\n",
       " (15, 0.3744466219568634),\n",
       " (16, 0.35619609606615177),\n",
       " (17, 0.3078563541643459),\n",
       " (18, 0.2972318344094391),\n",
       " (19, 0.29043613009673497),\n",
       " (20, 0.2799032149947495),\n",
       " (21, 0.26789950359321096),\n",
       " (22, 0.2948640647035681),\n",
       " (23, 0.28186945535927654),\n",
       " (24, 0.26889566145580374),\n",
       " (25, 0.26014449404421297),\n",
       " (26, 0.2570112268195403),\n",
       " (27, 0.2681514606590825),\n",
       " (28, 0.2764433863136213),\n",
       " (29, 0.27703424864496695),\n",
       " (30, 0.24763565029469709),\n",
       " (31, 0.27853450122827234),\n",
       " (32, 0.24590810414444325),\n",
       " (33, 0.24098000240979414),\n",
       " (34, 0.2535597621901794),\n",
       " (35, 0.2606798249031276),\n",
       " (36, 0.2353039120644176),\n",
       " (37, 0.24220635149031566),\n",
       " (38, 0.2381620625636712),\n",
       " (39, 0.27034918329935825),\n",
       " (40, 0.24504390191645095),\n",
       " (41, 0.24552797920225822),\n",
       " (42, 0.24682620744698405),\n",
       " (43, 0.2305389835115422),\n",
       " (44, 0.23952795446013955),\n",
       " (45, 0.23827619881980794),\n",
       " (46, 0.23632875522071142),\n",
       " (47, 0.23655228094269673),\n",
       " (48, 0.24178672659816758),\n",
       " (49, 0.2443902833032626),\n",
       " (50, 0.23530270416960938),\n",
       " (51, 0.25397426051840455),\n",
       " (52, 0.2596580304840903),\n",
       " (53, 0.2496277480067208),\n",
       " (54, 0.22394432332856085),\n",
       " (55, 0.23731502090223988),\n",
       " (56, 0.23996091777359263),\n",
       " (57, 0.23179299681933654),\n",
       " (58, 0.2369168378457808),\n",
       " (59, 0.2493837393530836),\n",
       " (60, 0.2538364349624337),\n",
       " (61, 0.25366050930782263),\n",
       " (62, 0.24201167443091018),\n",
       " (63, 0.24631076632990603),\n",
       " (64, 0.23315051283699137),\n",
       " (65, 0.23696946009489875),\n",
       " (66, 0.24055828936084914),\n",
       " (67, 0.2439566517520063),\n",
       " (68, 0.22238173510473247),\n",
       " (69, 0.2374168579436198),\n",
       " (70, 0.2256886599930384),\n",
       " (71, 0.23495797507513513),\n",
       " (72, 0.23108422231599726),\n",
       " (73, 0.24325025974476133),\n",
       " (74, 0.23609285481806522),\n",
       " (75, 0.23657394531221212),\n",
       " (76, 0.23582558745391183),\n",
       " (77, 0.23567321055006113),\n",
       " (78, 0.2196247053247582),\n",
       " (79, 0.2369205516660336),\n",
       " (80, 0.22798370733627438),\n",
       " (81, 0.22427767559820658),\n",
       " (82, 0.2226099610848262),\n",
       " (83, 0.22375030917673447),\n",
       " (84, 0.21551675265167575),\n",
       " (85, 0.2343391842831932),\n",
       " (86, 0.22492373171957905),\n",
       " (87, 0.2226844079275493),\n",
       " (88, 0.227025995795839),\n",
       " (89, 0.23051016504719157),\n",
       " (90, 0.23155096402378986),\n",
       " (91, 0.22285028569522095),\n",
       " (92, 0.215825896000904),\n",
       " (93, 0.21631111803513683),\n",
       " (94, 0.22718811664393262),\n",
       " (95, 0.21374505019640103),\n",
       " (96, 0.2085609879332786),\n",
       " (97, 0.21777691623076065),\n",
       " (98, 0.2205720025544442),\n",
       " (99, 0.21700497555102544)]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "import matplotlib.animation as animation\n",
    "#defining a class for matrix factorisation\n",
    "class MatFactor():\n",
    "    #overiding the init function by adding the following parameters as arguments (the function self, Ratings, number of workers, learning rates/weights, #iterations)\n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        #assigning the values of variables \n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "        # real user as x, item as y and rating as R\n",
    "        #we need to predict the values in R that aren't assigned by using the non-zero values in R\n",
    "        xs, ys = self.R.nonzero()\n",
    "        self.z = [self.R[x, y] for x, y in zip(xs, ys)]\n",
    "        self.x = [0 for i in range(len(self.z))]\n",
    "        self.y = [i for i in range(len(self.z))]\n",
    "        self.realLine = np.array([self.x, self.y, self.z])\n",
    "        self.log = [self.realLine]\n",
    "        self.shiftXIndex = 0\n",
    "\n",
    "    def getShiftX(self):\n",
    "        self.shiftXIndex += .5\n",
    "        return [self.shiftXIndex for i in range(len(self.z))]\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrix\n",
    "        self.P = np.random.normal(\n",
    "            scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(\n",
    "            scale=1./self.K, size=(self.num_items, self.K))\n",
    "\n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        # general_mean of non zero values\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "\n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "\n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            mse = self.mse()\n",
    "            training_process.append((i, mse))\n",
    "            print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "\n",
    "        return training_process\n",
    "    #defining function for mean square error\n",
    "    def mse(self):\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        newPredictLine = np.array(\n",
    "            [self.getShiftX(), self.y, [predicted[x, y] for x, y in zip(xs, ys)]])\n",
    "\n",
    "        self.log.append(newPredictLine)\n",
    "        error = 0\n",
    "        \n",
    "        #using for loop for calculating mean square error\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "    \n",
    "    #function for performing stochastic gradient descent\n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            self.b_u[i] += self.alpha * (2*e - self.beta * self.b_u[i]) #PERFORM UPDATION\n",
    "            self.b_i[j] += self.alpha * (2*e - self.beta * self.b_i[j])\n",
    "\n",
    "            self.P[i, :] += self.alpha * \\\n",
    "                (e * self.Q[j, :] - self.beta * self.P[i, :])\n",
    "            self.Q[j, :] += self.alpha * \\\n",
    "                (e * self.P[i, :] - self.beta * self.Q[j, :])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        prediction = self.b + self.b_u[i] + \\\n",
    "            self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "\n",
    "    def full_matrix(self):\n",
    "        return self.b + self.b_u[:, np.newaxis] + self.b_i[np.newaxis:, ] + self.P.dot(self.Q.T)\n",
    "\n",
    "\n",
    "R = np.random.randint(6, size=(5, 5)) #GENERATE Ratings at random\n",
    "mf = MatFactor(R, K=2, alpha=0.1, beta=0.01, iterations=100) #TRIAL-AND-ERROR FOR THESE HYPERPARAMETERS\n",
    "\n",
    "mf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b57kchAEuxcT"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MatrixFactorizationDSGD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
